---
title: "Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning Strategy."
collection: publications
permalink: /publication/2023-paper2
excerpt: '本文首次提出动态无数据知识蒸馏方法。CCF-B，SCI一区，IF=8.1'
date: 2023-06-02
venue: 'Information Sciences'
paperurl: 'http://ljrprocc.github.io/files/paper_ins.pdf'
citation: 'Jingru, Li. (2023). &quot;Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning Strategy.&quot; <i>Information Sciences</i>. 1(2).'
---

本文介绍
======
Data-free knowledge distillation (DFKD) is a widely-used strategy for Knowledge Distillation (KD) whose training data is not available. It trains a lightweight student model with the aid of a large pretrained teacher model without any access to training data. However, existing DFKD methods suffer from inadequate and unstable training process, as they do not adjust the generation target dynamically based on the status of the student model during learning. To address this limitation, we propose a novel DFKD method called CuDFKD. It teaches students by a dynamic strategy that gradually generates easy-to-hard pseudo samples, mirroring how humans learn. Besides, CuDFKD adapts the generation target dynamically according to the status of student model. Moreover, we provide a theoretical analysis of the majorization minimization (MM) algorithm and explain the convergence of CuDFKD. To measure the robustness and fidelity of DFKD methods, we propose two more metrics, and experiments shows CuDFKD has comparable performance to state-of-the-art (SOTA) DFKD methods on all datasets. Experiments also present that our CuDFKD has the fastest convergence and best robustness over other SOTA DFKD methods.

[Download paper here](http://ljrprocc.github.io/files/paper_ins.pdf)

Recommended citation: Jingru, Li. (2023). "Dynamic Data-Free Knowledge Distillation by Easy-to-Hard Learning Strategy." <i>Information Sciences</i>. 1(2).